# How to Run A/B Test (Gen 0 vs Gen 1)

**Prerequisites:**
- âœ… Generation 0 baseline complete
- âœ… Fine-tuned model trained and deployed
- â³ Fine-tuned model ID available

---

## Quick Test: Single Seed Comparison

Before running full validation, test the fine-tuned model on a single problematic seed:

### Test Case: "to meet" FCFS Pattern

**Seed:** S0022 from Spanish course
**Problem in Gen 0:** Maps "to meet" â†’ "conocer" (FCFS violation)
**Expected in Gen 1:** Maps "to meet someone" â†’ "conocer a alguien" (chunked up)

```python
import anthropic
import os

client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])

# Test with Gen 1 (fine-tuned) model
model_id = "claude-sonnet-4-5:ssi-lego-v1:YOUR_MODEL_ID"

message = client.messages.create(
    model=model_id,
    max_tokens=2048,
    system="""You are an expert at extracting LEGO pairs from language learning translations.

You follow two critical rules:

1. FD (Functionally Deterministic): Learner sees known chunk â†’ knows exactly ONE target response
2. FCFS (First Come First Served): First occurrence of known chunk claims the mapping. Subsequent occurrences must chunk up with context.

You classify LEGOs as BASE, COMPOSITE, or FEEDER.""",
    messages=[{
        "role": "user",
        "content": """Extract LEGO pairs from this translation:

**Seed:** S0022
**Target Language:** Spanish
**Known Language:** English

**Target:** Quiero conocer a alguien nuevo.
**Known:** I want to meet someone new.

**Context:** The known chunk "to meet" has already been mapped to "encontrarnos" (meet/get together) in a previous seed (S0018).

Task: Extract LEGO pairs following FD + FCFS rules."""
    }]
)

print(message.content)
```

**Look for:**
- âœ… Did it chunk up? ("to meet someone" instead of "to meet")
- âœ… Did it mention FCFS in reasoning?
- âœ… Did it avoid the violation?

---

## Full A/B Test Process

### Step 1: Run Validation on Gen 1 Outputs

**Option A: Re-run full course generation with Gen 1 model**

This requires updating the automation server to use the fine-tuned model, then regenerating courses. Complex but most accurate.

**Option B: Validate existing courses as-is** (Faster)

If your existing approved courses are "good enough", just run the validator to establish a comparison point:

```bash
# Run validator on all 4 courses
node skills/lego-extraction-skill/scripts/validate-fd-fcfs.cjs --all

# This will give you the actual quality scores
# Save output to generation-1-results.json (manually for now)
```

**Option C: Test on subset of seeds** (Quickest proof)

Generate LEGOs for just the 4 corrected seeds using Gen 1 model and compare:

```python
# Test the 4 specific seeds that had corrections
test_seeds = [
    {"id": "S0022", "course": "spa_for_eng_30seeds", "pattern": "to meet"},
    {"id": "S0022", "course": "ita_for_eng_30seeds", "pattern": "to meet"},
    {"id": "S0022", "course": "cmn_for_eng_30seeds", "pattern": "to meet"},
    {"id": "S0010", "course": "cmn_for_eng_30seeds", "pattern": "to remember"},
]

# For each seed, ask Gen 1 model to extract LEGOs
# Check if violations disappeared
```

---

### Step 2: Create generation-1-results.json

Format matches `generation-0-baseline.json`:

```json
{
  "generation": 1,
  "model": "claude-sonnet-4-5:ssi-lego-v1:YOUR_MODEL_ID",
  "created_at": "2025-10-18T15:00:00.000Z",
  "description": "Generation 1 results after fine-tuning on 453 examples",
  "overall_metrics": {
    "average_quality_score": 92.5,
    "total_courses": 4,
    "total_seeds": 120,
    "total_legos": 361,
    "total_issues": 12,
    "critical_issues": 2,
    "high_issues": 0,
    "medium_issues": 10,
    "low_issues": 0
  },
  "by_course": [
    {
      "course_name": "spa_for_eng_30seeds",
      "language_pair": "Spanish for English",
      "quality_score": 95.0,
      "total_seeds": 30,
      "total_legos": 89,
      "issues": {
        "total": 3,
        "critical": 0,
        "high": 0,
        "medium": 3,
        "low": 0,
        "by_type": {
          "CLASSIFICATION_ERROR": 3
        }
      },
      "fcfs_violations": []
    }
    // ... more courses
  ]
}
```

**Key fields:**
- `average_quality_score`: Should be 90%+ (target improvement)
- `critical_issues`: Should be â‰¤4 (down from 16)
- `fcfs_violations`: Should be empty or minimal for corrected patterns

---

### Step 3: Run Comparison

```bash
node skills/lego-extraction-skill/testing/compare-generations.cjs
```

This will:
- Load Gen 0 baseline
- Load Gen 1 results
- Calculate improvements
- Check self-healing
- Verify success criteria
- Generate comparison report

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GENERATION 0 vs GENERATION 1 COMPARISON
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OVERALL QUALITY
  Gen 0: 74.0%
  Gen 1: 92.5%
  Improvement: +18.5% (25.0% relative)

FCFS VIOLATIONS
  Gen 0: 16
  Gen 1: 2
  Reduction: -14 (-87.5%)

SELF-HEALING VERIFICATION

  "to meet" FCFS pattern:
    Gen 0: 4 violations
    Gen 1: 0 violations
    Status: âœ… HEALED
    ğŸ¯ Model learned this pattern!

  "to remember" FCFS pattern:
    Gen 0: 3 violations
    Gen 1: 0 violations
    Status: âœ… HEALED
    ğŸ¯ Model learned this pattern!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ‰ ALL SUCCESS CRITERIA MET!
Recursive up-regulation PROVEN âœ…
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Interpreting Results

### SUCCESS: Recursive Up-Regulation Proven âœ…

If you see:
- âœ… Quality improved by â‰¥10%
- âœ… FCFS violations reduced by â‰¥50%
- âœ… At least 2 corrected patterns healed
- âœ… Model generalizes across languages

**Conclusion:** Agents CAN learn from experience through fine-tuning!

### PARTIAL SUCCESS: Improvement but not full healing

If you see:
- âœ… Quality improved
- âš ï¸  Some but not all patterns healed

**Next steps:**
- Add more correction examples
- Fine-tune Generation 2
- Iterate until all patterns heal

### FAILURE: No improvement

If you see:
- âŒ Quality didn't improve
- âŒ Patterns still recur

**Debug:**
- Check fine-tuning completed successfully
- Verify training data format
- Check model ID is correct
- Review training/validation loss curves

---

## What Success Proves

If the A/B test shows:
1. **Quality improves** â†’ Fine-tuning works
2. **Errors reduce** â†’ Model learned rules
3. **Patterns healed** â†’ Self-healing works
4. **Cross-language** â†’ Generalizes concepts

**Then we've proven:**
- Agents can learn from experience âœ…
- Recursive up-regulation works âœ…
- System can approach autonomy âœ…
- AI OS is real, not theoretical âœ…

---

## Timeline

| Step | Duration |
|------|----------|
| Fine-tuning completes | (waiting) |
| Quick test (single seed) | 5 min |
| Generate Gen 1 results | 30 min - 2 hours |
| Run comparison | 1 min |
| Document proof | 30 min |

**Total:** ~1-3 hours after fine-tuning completes

---

## After Proving It Works

### Document the Proof
1. Screenshot of comparison output
2. Update RECURSIVE_UPREGULATION_STATUS.md
3. Create PROOF_OF_CONCEPT.md with results

### Plan Generation 2
1. Identify remaining errors from Gen 1
2. Add corrections to training dataset
3. Fine-tune Gen 2 model
4. Expect another quality jump

### Scale the System
1. Apply to other phases (baskets, introductions)
2. Scale to 668-seed courses
3. Track metrics through Gen 50
4. Achieve 99.9% autonomy

---

**The comparison framework is ready. When fine-tuning completes, we prove recursive up-regulation works.** ğŸ¯
